{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeaAR0/Scalable_Quiz_Website/blob/main/Lec_03_Lab_01_Perceptron(with_Answer).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ6kynxJ0jvC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the perceptron model\n",
        "# class Perceptron(nn.Module): This creates a new class called Perceptron, which is a subclass of nn.Module. By inheriting from nn.Module, this class can take advantage of various neural network-related functions in PyTorch.\n",
        "class Perceptron(nn.Module):\n",
        "    def __init__(self):\n",
        "        # super(Perceptron, self).init(): This calls the initialization method of the parent class nn.Module, ensuring the class inherits all its methods and attributes.\n",
        "        super(Perceptron, self).__init__()\n",
        "        # self.fc = nn.Linear(2, 1): This defines a fully connected (linear) layer with two inputs and one output. This layer is the core of the perceptron model, which takes two inputs (as needed for logic gates like AND, OR, XOR) and computes a single output.\n",
        "        self.fc = nn.Linear(2, 1)  # 2 inputs, 1 output\n",
        "\n",
        "    # def forward(self, x): This defines the forward pass of the perceptron model. During the forward pass, data is passed through the model, and the model makes predictions.\n",
        "    def forward(self, x):\n",
        "        # self.fc(x): The input x (which is a tensor of size [2]) is passed through the fully connected layer fc. This computes the weighted sum of inputs:\n",
        "        return torch.sigmoid(self.fc(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input data for AND, OR, and XOR gates (2 inputs)\n",
        "inputs = torch.FloatTensor([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "])\n",
        "\n",
        "# Target outputs for AND, OR, and XOR gates\n",
        "targets_and = torch.FloatTensor([[0], [0], [0], [1]])  # AND gate\n",
        "targets_or = torch.FloatTensor([[0], [1], [1], [1]])   # OR gate\n",
        "targets_xor = torch.FloatTensor([[0], [1], [1], [0]])  # XOR gate\n"
      ],
      "metadata": {
        "id": "EV3ABXZ12XC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_perceptron(perceptron, inputs, targets, epochs=1000, lr=0.1):\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
        "    optimizer = torch.optim.SGD(perceptron.parameters(), lr=lr)  # Stochastic Gradient Descent\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        outputs = perceptron(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (epoch+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Create a perceptron model instance\n",
        "perceptron_and = Perceptron()\n",
        "perceptron_or = Perceptron()\n",
        "perceptron_xor = Perceptron()\n",
        "\n",
        "# Train the model on AND gate data\n",
        "print(\"Training on AND gate:\")\n",
        "train_perceptron(perceptron_and, inputs, targets_and)\n",
        "\n",
        "# Train the model on OR gate data\n",
        "print(\"\\nTraining on OR gate:\")\n",
        "train_perceptron(perceptron_or, inputs, targets_or)\n",
        "\n",
        "# Train the model on XOR gate data\n",
        "print(\"\\nTraining on XOR gate:\")\n",
        "train_perceptron(perceptron_xor, inputs, targets_xor)\n"
      ],
      "metadata": {
        "id": "YqBL7AnY2dm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_perceptron(perceptron, inputs):\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        outputs = perceptron(inputs)\n",
        "        predicted = outputs.round()  # Round the output to get binary classification\n",
        "        print(\"Predictions:\", predicted.numpy())\n",
        "\n",
        "# Test the trained perceptron on AND gate\n",
        "print(\"\\nTesting on AND gate:\")\n",
        "test_perceptron(perceptron_and, inputs)\n",
        "\n",
        "# Test the trained perceptron on OR gate\n",
        "print(\"\\nTesting on OR gate:\")\n",
        "test_perceptron(perceptron_or, inputs)\n",
        "\n",
        "# Test the trained perceptron on XOR gate\n",
        "print(\"\\nTesting on XOR gate:\")\n",
        "test_perceptron(perceptron_xor, inputs)\n"
      ],
      "metadata": {
        "id": "r5Q_DO962jTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now, let's implement AND, OR, and XOR gates using a Multi-Layer Perceptron (MLP).\n",
        "# Below is a multi-layer perceptron."
      ],
      "metadata": {
        "id": "ECsw46pN4cct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(2, 10),  # Input layer to hidden layer (2 inputs, 10 hidden nodes)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(10, 1),  # Hidden layer to output layer (1 output)\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "hRjJRc1y046A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
        "optimizer = torch.optim.SGD(mlp.parameters(), lr=0.1)  # Stochastic Gradient Descent optimizer\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in data_loader:\n",
        "        # Forward pass\n",
        "        outputs = mlp(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print loss every 100 epochs\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "id": "4naPJZ6h7zfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_MLP(mlp, inputs, targets, epochs=1000, lr=0.1):\n",
        "\n",
        "    YOUR CODE HERE\n",
        "    ..."
      ],
      "metadata": {
        "id": "mpyam-Ly2WZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fzdh4fJ-6Fxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model with a Simple Dataset\n",
        "- Use a basic dataset like XOR or a toy classification dataset from sklearn.datasets\n"
      ],
      "metadata": {
        "id": "aOtekfw86GJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.datasets import make_moons\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Create a simple dataset\n",
        "# This function generates a synthetic 2D dataset with two interleaving half circles, commonly used for classification problems. The noise=0.1 parameter adds some variability to the data.\n",
        "X, y = make_moons(n_samples=100, noise=0.1)\n",
        "# X_tensor: This converts the datasetâ€™s features into a PyTorch tensor.\n",
        "X_tensor = torch.FloatTensor(X)\\\n",
        "# This converts the target labels into a PyTorch tensor. We reshape the target labels with .view(-1, 1) to match the expected shape for binary classification (a column vector).\n",
        "y_tensor = torch.FloatTensor(y).view(-1, 1)\n",
        "# TensorDataset: Combines the features and labels into a dataset that PyTorch can use.\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "# DataLoader: Loads the dataset in mini-batches of size 10 and shuffles it to ensure that the data is randomly distributed in each epoch.\n",
        "data_loader = DataLoader(dataset, batch_size=10, shuffle=True)\n"
      ],
      "metadata": {
        "id": "zzhf9o-J6N3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(2, 10),  # Input layer (2 inputs) to hidden layer (10 hidden units)\n",
        "            nn.ReLU(),         # ReLU activation function\n",
        "            nn.Linear(10, 1),  # Hidden layer (10 hidden units) to output layer (1 output)\n",
        "            nn.Sigmoid()       # Sigmoid activation for binary classification\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Initialize the MLP model\n",
        "mlp = MLP()\n"
      ],
      "metadata": {
        "id": "kHkk2sPG6euV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
        "optimizer = torch.optim.SGD(mlp.parameters(), lr=0.1)  # Stochastic Gradient Descent optimizer\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in data_loader:\n",
        "        # Forward pass\n",
        "        outputs = mlp(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print loss every 100 epochs\n",
        "    if (epoch+1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ],
      "metadata": {
        "id": "FWk7SEb9704B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_mlp(mlp, inputs):\n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        outputs = mlp(inputs)\n",
        "        predicted = outputs.round()  # Round the output to get binary predictions\n",
        "        return predicted\n",
        "\n",
        "# Test the trained MLP on the dataset\n",
        "predictions = test_mlp(mlp, X_tensor)\n",
        "print(\"Predictions:\\n\", predictions.numpy())\n"
      ],
      "metadata": {
        "id": "BSrQaiAE75TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to plot the decision boundary\n",
        "def plot_decision_boundary(model, X, y):\n",
        "    # Define a grid of points to plot the decision boundary\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "    xx, yy = torch.meshgrid(torch.arange(x_min, x_max, 0.01), torch.arange(y_min, y_max, 0.01))\n",
        "    grid = torch.cat([xx.reshape(-1, 1), yy.reshape(-1, 1)], dim=1)\n",
        "\n",
        "    # Get model predictions for each point in the grid\n",
        "    with torch.no_grad():\n",
        "        Z = model(grid).round().reshape(xx.shape)\n",
        "\n",
        "    # Plot the decision boundary\n",
        "    plt.contourf(xx, yy, Z, cmap='Paired')\n",
        "\n",
        "    # Plot the original data points\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, edgecolor='k', cmap='Paired')\n",
        "    plt.title('Decision Boundary')\n",
        "    plt.show()\n",
        "\n",
        "# Plot decision boundary and data points\n",
        "plot_decision_boundary(mlp, X_tensor.numpy(), y)\n"
      ],
      "metadata": {
        "id": "Jo_4fU5s77QR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}